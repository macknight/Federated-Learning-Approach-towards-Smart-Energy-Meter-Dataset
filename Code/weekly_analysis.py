# -*- coding: utf-8 -*-
"""Weekly_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/macknight/Federated-Learning-Approach-towards-Smart-Energy-Meter-Dataset/blob/Draft/Code/Weekly_analysis.ipynb
"""

#@test {"skip": true}

# !pip install --quiet --upgrade tensorflow-federated

import collections

import numpy as np
import tensorflow as tf
import tensorflow_federated as tff

np.random.seed(0)

tff.federated_computation(lambda: 'Hello, World!')()

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

data = pd.read_csv('/content/drive/MyDrive/LAB3-20221109T160610Z-001/LCL-FullData/Cluster18data.csv')

data['DateTime'] = pd.to_datetime(data['DateTime'])

# Filter the DataFrame to include readings from Jan 01, 2012, to Dec 31, 2012
start_date1 = pd.to_datetime('2011-10-01')
end_date1 = pd.to_datetime('2013-02-28')
filtered_data1 = data[(data['DateTime'] >= start_date1) & (data['DateTime'] <= end_date1)]

# Select 20 unique LCLids
lclid_list1 = filtered_data1['LCLid'].unique()
selected_lclids1 = lclid_list1[:20]

# Filter data for the selected LCLids
f_data1 = filtered_data1[filtered_data1['LCLid'].isin(selected_lclids1)]

datan1 = f_data1.copy()
datan1['KWH/hh'] = datan1['KWH/hh'].astype(np.float32)


datan1 = datan1.drop('cluster', axis=1)
datan1 = datan1.drop('stdorToU', axis=1)

datan1.reset_index(drop=True, inplace=True)

datan1['DateTime'] = pd.to_datetime(datan1.DateTime).dt.tz_localize(None)
for i in range(len(datan1)):
  datan1['DateTime'][i]=datan1['DateTime'][i].timestamp()

datan1['DateTime'] = datan1['DateTime'].astype(np.float32)

# Sort the data by 'LCLid' and 'DateTime'
datan1.sort_values(['LCLid', 'DateTime'], inplace=True)

# Define the client window dataset function for a specific LCLid
def create_client_dataset_for_LCLid(client_data, window_size, step_size):
    client_windows = []
    client_targets = []
    num_readings = len(client_data)

    # Iterate over the readings using the sliding window
    for i in range(0, num_readings - window_size, step_size):
        window_start = i
        window_end = i + window_size - 1
        prediction_index = window_end + step_size

        # Extract the window and the prediction target
        window = client_data.iloc[window_start:window_end + 1]['KWH/hh'].values
        target = client_data.iloc[prediction_index]['KWH/hh']

        client_windows.append(window)
        client_targets.append(target)

    # Create an ordered dictionary with 'x' and 'y' keys
    ordered_dict = collections.OrderedDict()
    ordered_dict['x'] = tf.stack(client_windows)
    ordered_dict['y'] = tf.expand_dims(client_targets, axis=-1)


    return ordered_dict

window_size = 336
step_size = 1

# Filter the dataframe for the specific LCLid
example_LCLid = datan1['LCLid'].unique()[3]
clientyy_data = datan1[datan1['LCLid'] == example_LCLid]

# Create the client dataset for the specific LCLid
example_client_dataset = create_client_dataset_for_LCLid(clientyy_data, window_size, step_size)

print("Client dataset for LCLid", example_LCLid)
print(example_client_dataset)

NUM_EPOCHS = 5
BATCH_SIZE = 12
SHUFFLE_BUFFER = 60
PREFETCH_BUFFER = 6

def preprocess_client_dataset(dataset):
    def batch_format_fn(element):
        return collections.OrderedDict(
            x=tf.reshape(element['x'], [-1, 336]),
            y=tf.reshape(element['y'], [-1, 1]))
    return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(
        BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)


preprocessed_example_client_dataset = preprocess_client_dataset(tf.data.Dataset.from_tensor_slices(example_client_dataset))


sample_batch = tf.nest.map_structure(lambda x: x.numpy(),
                                     next(iter(preprocessed_example_client_dataset)))

sample_batch

import random

NUM_CLIENTS = 12  # Replace with desired number of clients
all_clients = datan1['LCLid'].unique()
sample_clients = random.sample(all_clients.tolist(), NUM_CLIENTS)

sample_clients_list = sample_clients
sample_clients_list

# Iterate over unique LCLids in the dataframe
client_datasets_12 = {}
for LCLid in sample_clients_list:
    # Filter the dataframe for the current LCLid
    client_data = datan1[datan1['LCLid'] == LCLid]

    clientxx_dataset = create_client_dataset_for_LCLid(client_data, window_size, step_size)

    # Create the client dataset for the current LCLid
    preprocessed_client_dataset = preprocess_client_dataset(tf.data.Dataset.from_tensor_slices(clientxx_dataset))

    # Extract a sample batch from the preprocessed dataset
    sam_batch = tf.nest.map_structure(lambda x: x.numpy(), next(iter(preprocessed_client_dataset)))

    # Store the preprocessed dataset in the dictionary with LCLid as the key
    client_datasets_12[LCLid] = preprocessed_client_dataset

    print("Client dataset for LCLid", LCLid)
    print(sam_batch)

def make_federated_data(client_datasets, sample_clients_list):
    return [
        client_datasets[x] for x in sample_clients_list
    ]

federated_train_data_12 = make_federated_data(client_datasets_12, sample_clients)

print(f'Number of client datasets: {len(federated_train_data_12)}')
print(f'First dataset: {federated_train_data_12[0]}')
print(f'Second dataset: {federated_train_data_12[1]}')

preprocessed_example_client_dataset.element_spec

def create_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(16, activation='relu', input_shape=(336,)),
        tf.keras.layers.Dense(8, activation='relu'),
        tf.keras.layers.Dense(4, activation='relu'),
        tf.keras.layers.Dense(1)
    ])
    return model

def model_fn():
    keras_model = create_model()
    loss = tf.keras.losses.MeanAbsoluteError()
    tff_model = tff.learning.models.from_keras_model(
        keras_model,
        input_spec=preprocessed_example_client_dataset.element_spec,
        loss=loss,
        metrics=[tf.keras.metrics.RootMeanSquaredError()]
    )
    return tff_model

model = model_fn()
print(model)

# training starts
training_process = tff.learning.algorithms.build_weighted_fed_avg(
    model_fn,
    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.01),
    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))

print(training_process.initialize.type_signature.formatted_representation())

train_state = training_process.initialize()

result = training_process.next(train_state, federated_train_data_12)
train_state = result.state
train_metrics = result.metrics
print('round  1, metrics={}'.format(train_metrics))

NUM_ROUNDS = 21
for round_num in range(2, NUM_ROUNDS):
  result = training_process.next(train_state, federated_train_data_12)
  train_state = result.state
  train_metrics = result.metrics
  print('round {:2d}, metrics={}'.format(round_num, train_metrics))

import plotly.graph_objects as go

# Data for Cluster 08
rounds_1 = list(range(1, 21))
mae_1 = [
    0.06737575, 0.06293772, 0.060796063, 0.05940758, 0.058408998,
    0.057648093, 0.05703, 0.056526024, 0.05610915, 0.055751048,
    0.05542996, 0.055151854, 0.054903667, 0.05467346, 0.05447607,
    0.054296006, 0.054134406, 0.053993754, 0.05387066, 0.053758096
]

# Data for Cluster 09
rounds_2 = list(range(1, 21))
mae_2 = [
    0.07034567, 0.06416719, 0.061836798, 0.060558915, 0.059689056,
    0.059010174, 0.058474753, 0.058048673, 0.057682555, 0.057362556,
    0.057089813, 0.056855284, 0.056644276, 0.056458063, 0.05630099,
    0.056166247, 0.05603734, 0.055925187, 0.05582829, 0.05573636
]

# Creating trace for Cluster 08
trace1 = go.Scatter(
    x=rounds_1,
    y=mae_1,
    mode='lines',
    name='Cluster 08'
)

# Creating trace for Cluster 09
trace2 = go.Scatter(
    x=rounds_2,
    y=mae_2,
    mode='lines',
    name='Cluster 09'
)

# Creating data list
data = [trace1, trace2]

# Creating layout
layout = go.Layout(
    title='Mean Absolute Error (MAE) of Load Forecasting vs. Federated Rounds',
    xaxis=dict(title='Federated Rounds'),
    yaxis=dict(title='Training data - MAE')
)

# Creating figure
fig = go.Figure(data=data, layout=layout)

# Displaying the graph
fig.show()

import plotly.graph_objects as go

# Data for Cluster 04
rounds_1 = list(range(1, 21))
mae_1 = [
    0.11573566, 0.10361333, 0.09922377, 0.09694171, 0.09549518,
    0.094502784, 0.09377624, 0.09319854, 0.09278589, 0.09244496,
    0.0921646, 0.09189611, 0.09168941, 0.09149853, 0.09130679,
    0.09120082, 0.09106272, 0.09095054, 0.09086295, 0.090747595
]

# Data for Cluster 14
rounds_2 = list(range(1, 21))
mae_2 = [
    0.1708811, 0.1584751, 0.15332645, 0.15120173, 0.14984801,
    0.14892414, 0.14833829, 0.14782089, 0.14744286, 0.14694408,
    0.14646958, 0.14602049, 0.14571527, 0.14540415, 0.14500421,
    0.1446545, 0.14432915, 0.14402962, 0.14367345, 0.14337786
]

# Creating trace for Cluster 04
trace1 = go.Scatter(
    x=rounds_1,
    y=mae_1,
    mode='lines',
    name='Cluster 04'
)

# Creating trace for Cluster 14
trace2 = go.Scatter(
    x=rounds_2,
    y=mae_2,
    mode='lines',
    name='Cluster 14'
)

# Creating data list
data = [trace1, trace2]

# Creating layout
layout = go.Layout(
    title='Mean Absolute Error (MAE) of Load Forecasting vs. Federated Rounds',
    xaxis=dict(title='Federated Rounds'),
    yaxis=dict(title='Training data - MAE')
)

# Creating figure
fig = go.Figure(data=data, layout=layout)

# Displaying the graph
fig.show()

import plotly.graph_objects as go

# Data for Cluster 07
rounds_1 = list(range(1, 21))
mae_1 = [
    0.24215707, 0.22306563, 0.21767606, 0.21497612, 0.21307865,
    0.21194346, 0.21102037, 0.2100187, 0.20892245, 0.2077742,
    0.20704211, 0.20606637, 0.20509331, 0.20452215, 0.20378909,
    0.20307666, 0.2025013, 0.20195374, 0.20134038, 0.20096558
]

# Data for Cluster 18
rounds_2 = list(range(1, 21))
mae_2 = [
    0.25958988, 0.2282181, 0.21869427, 0.21352097, 0.21052141,
    0.20815776, 0.20585926, 0.2046754, 0.20356816, 0.20232281,
    0.20153011, 0.2007333, 0.20026353, 0.19920154, 0.19846562,
    0.19802547, 0.19739366, 0.19712806, 0.19636787, 0.19576551
]

# Creating trace for Cluster 07
trace1 = go.Scatter(
    x=rounds_1,
    y=mae_1,
    mode='lines',
    name='Cluster 07'
)

# Creating trace for Cluster 18
trace2 = go.Scatter(
    x=rounds_2,
    y=mae_2,
    mode='lines',
    name='Cluster 18'
)

# Creating data list
data = [trace1, trace2]

# Creating layout
layout = go.Layout(
    title='Mean Absolute Error (MAE) of Load Forecasting vs. Federated Rounds',
    xaxis=dict(title='Federated Rounds'),
    yaxis=dict(title='Training data - MAE')
)

# Creating figure
fig = go.Figure(data=data, layout=layout)

# Displaying the graph
fig.show()

import plotly.graph_objects as go

# Data for Cluster - 08, Batch Size = 24, Shuffle Buffer = 120, Prefetch Buffer = 12
rounds_1 = list(range(1, 16))
mae_1 = [
    0.07406851, 0.06911341, 0.06588133, 0.06381104, 0.062328234,
    0.061173238, 0.060279164, 0.059583493, 0.059030402, 0.058575064,
    0.05819059, 0.05786198, 0.05756866, 0.05733508, 0.057126228
]

# Data for Cluster - 08, Batch Size = 12, Shuffle Buffer = 60, Prefetch Buffer = 6
rounds_2 = list(range(1, 16))
mae_2 = [
    0.070309095, 0.06450078, 0.061607864, 0.060092628, 0.059076704,
    0.05836942, 0.057809025, 0.057371534, 0.05702302, 0.056733135,
    0.056459244, 0.056229576, 0.056033693, 0.055863913, 0.055711426
]

# Creating trace for Cluster - 08, Batch Size = 24, Shuffle Buffer = 120, Prefetch Buffer = 12
trace1 = go.Scatter(
    x=rounds_1,
    y=mae_1,
    mode='lines',
    name='Batch Size = 24, Shuffle Buffer = 120, Prefetch Buffer = 12'
)

# Creating trace for Cluster - 08, Batch Size = 12, Shuffle Buffer = 60, Prefetch Buffer = 6
trace2 = go.Scatter(
    x=rounds_2,
    y=mae_2,
    mode='lines',
    name='Batch Size = 12, Shuffle Buffer = 60, Prefetch Buffer = 6'
)

# Creating data list
data = [trace1, trace2]

# Creating layout
layout = go.Layout(
    title='Mean Absolute Error (MAE) vs. Federated Rounds with Learning Rate - 0.02',
    xaxis=dict(title='Federated Rounds'),
    yaxis=dict(title='MAE')
)

# Creating figure
fig = go.Figure(data=data, layout=layout)

# Displaying the graph
fig.show()

import plotly.graph_objects as go

# Data for Batch Size = 12, Shuffle Buffer = 60, Prefetch Buffer = 6, Learning Rate = 0.02
rounds_1 = list(range(1, 16))
mae_1 = [
    0.070309095, 0.06450078, 0.061607864, 0.060092628, 0.059076704,
    0.05836942, 0.057809025, 0.057371534, 0.05702302, 0.056733135,
    0.056459244, 0.056229576, 0.056033693, 0.055863913, 0.055711426
]

# Data for Batch Size = 12, Shuffle Buffer = 60, Prefetch Buffer = 6, Learning Rate = 0.01
rounds_2 = list(range(1, 16))
mae_2 = [
    0.06575257, 0.059883647, 0.05776895, 0.056706358, 0.056026068,
    0.055522144, 0.05512807, 0.05480615, 0.054533303, 0.05431394,
    0.054116458, 0.053937282, 0.053791393, 0.05366001, 0.05353039
]

# Creating trace for Batch Size = 12, Shuffle Buffer = 60, Prefetch Buffer = 6, Learning Rate = 0.02
trace1 = go.Scatter(
    x=rounds_1,
    y=mae_1,
    mode='lines',
    name='Learning Rate = 0.02'
)

# Creating trace for Batch Size = 12, Shuffle Buffer = 60, Prefetch Buffer = 6, Learning Rate = 0.01
trace2 = go.Scatter(
    x=rounds_2,
    y=mae_2,
    mode='lines',
    name='Learning Rate = 0.01'
)

# Creating data list
data = [trace1, trace2]

# Creating layout
layout = go.Layout(
    title='Mean Absolute Error (MAE) vs. Federated Rounds',
    xaxis=dict(title='Federated Rounds'),
    yaxis=dict(title='MAE')
)

# Creating figure
fig = go.Figure(data=data, layout=layout)

# Displaying the graph
fig.show()

"""FEDERATED EVALUATION"""

# Filter data for the specified time period
start_date = '2013-03-01'
end_date = '2014-02-28'
filtered_data = data[(data['DateTime'] >= start_date) & (data['DateTime'] <= end_date)]

# Calculate the average KWH/hh for each LCLid
average_kwh = filtered_data.groupby('LCLid')['KWH/hh'].mean()

# Find the LCLids with the lowest, highest, and medium average KWH/hh
lowest_avg_lclid = average_kwh.idxmin()
highest_avg_lclid = average_kwh.idxmax()
medium_avg_lclid = average_kwh.sort_values().index[len(average_kwh) // 2]

lowest_avg_lclid, highest_avg_lclid, medium_avg_lclid

# Assuming your data is stored in a DataFrame called 'data'
# Calculate the average KWH/hh for each LCLid
average_kwh = filtered_data.groupby('LCLid')['KWH/hh'].mean()

# Print all LCLids with their corresponding average KWH/hh
for lclid, avg_kwh in average_kwh.items():
    print(f"LCLid: {lclid}, Average KWH/hh: {avg_kwh}")

filtered_data44 = data[data['LCLid'] == 'MAC004593']

# Filter the DataFrame to include readings from Jan 01, 2014, to Jan 31, 2014
teststart_date1 = pd.to_datetime('2014-02-14')
testend_date1 = pd.to_datetime('2014-02-21')
test_data1 = filtered_data44[(filtered_data44['DateTime'] >= teststart_date1) & (filtered_data44['DateTime'] <= testend_date1)]

teststart_date2 = pd.to_datetime('2014-02-15')
testend_date2 = pd.to_datetime('2014-02-22')
test_data2 = filtered_data44[(filtered_data44['DateTime'] >= teststart_date2) & (filtered_data44['DateTime'] <= testend_date2)]


teststart_date3 = pd.to_datetime('2014-02-16')
testend_date3 = pd.to_datetime('2014-02-23')
test_data3 = filtered_data44[(filtered_data44['DateTime'] >= teststart_date3) & (filtered_data44['DateTime'] <= testend_date3)]

teststart_date4 = pd.to_datetime('2014-02-17')
testend_date4 = pd.to_datetime('2014-02-24')
test_data4 = filtered_data44[(filtered_data44['DateTime'] >= teststart_date4) & (filtered_data44['DateTime'] <= testend_date4)]

teststart_date5 = pd.to_datetime('2014-02-18')
testend_date5 = pd.to_datetime('2014-02-25')
test_data5 = filtered_data44[(filtered_data44['DateTime'] >= teststart_date5) & (filtered_data44['DateTime'] <= testend_date5)]

teststart_date6 = pd.to_datetime('2014-02-19')
testend_date6 = pd.to_datetime('2014-02-26')
test_data6 = filtered_data44[(filtered_data44['DateTime'] >= teststart_date6) & (filtered_data44['DateTime'] <= testend_date6)]

teststart_date7 = pd.to_datetime('2014-02-20')
testend_date7 = pd.to_datetime('2014-02-27')
test_data7 = filtered_data44[(filtered_data44['DateTime'] >= teststart_date7) & (filtered_data44['DateTime'] <= testend_date7)]

# teststart_date8 = pd.to_datetime('2013-10-01')
# testend_date8 = pd.to_datetime('2013-10-31')
# test_data8 = filtered_data44[(filtered_data44['DateTime'] >= teststart_date8) & (filtered_data44['DateTime'] <= testend_date8)]

# teststart_date9 = pd.to_datetime('2013-11-01')
# testend_date9 = pd.to_datetime('2013-11-30')
# test_data9 = filtered_data44[(filtered_data44['DateTime'] >= teststart_date9) & (filtered_data44['DateTime'] <= testend_date9)]

# teststart_date10 = pd.to_datetime('2013-12-01')
# testend_date10 = pd.to_datetime('2013-12-31')
# test_data10 = filtered_data44[(filtered_data44['DateTime'] >= teststart_date10) & (filtered_data44['DateTime'] <= testend_date10)]

# teststart_date11 = pd.to_datetime('2014-01-01')
# testend_date11 = pd.to_datetime('2014-01-31')
# test_data11 = filtered_data44[(filtered_data44['DateTime'] >= teststart_date11) & (filtered_data44['DateTime'] <= testend_date11)]

# teststart_date12 = pd.to_datetime('2014-02-01')
# testend_date12 = pd.to_datetime('2014-02-28')
# test_data12 = filtered_data44[(filtered_data44['DateTime'] >= teststart_date12) & (filtered_data44['DateTime'] <= testend_date12)]

test_data5

# Select unique LCLids
lclid_lt1 = test_data1['LCLid'].unique()
sel_lclids1 = lclid_lt1[:1]
# Filter data for the selected LCLids
ft_data1 = test_data1[test_data1['LCLid'].isin(sel_lclids1)]

# Select unique LCLids
lclid_lt2 = test_data2['LCLid'].unique()
sel_lclids2 = lclid_lt2[:1]
# Filter data for the selected LCLids
ft_data2 = test_data2[test_data2['LCLid'].isin(sel_lclids2)]

# Select unique LCLids
lclid_lt3 = test_data3['LCLid'].unique()
sel_lclids3 = lclid_lt3[:1]
# Filter data for the selected LCLids
ft_data3 = test_data3[test_data3['LCLid'].isin(sel_lclids3)]

# Select unique LCLids
lclid_lt4 = test_data4['LCLid'].unique()
sel_lclids4 = lclid_lt4[:1]
# Filter data for the selected LCLids
ft_data4 = test_data4[test_data4['LCLid'].isin(sel_lclids4)]

# Select unique LCLids
lclid_lt5 = test_data5['LCLid'].unique()
sel_lclids5 = lclid_lt5[:1]
# Filter data for the selected LCLids
ft_data5 = test_data5[test_data5['LCLid'].isin(sel_lclids5)]

# Select unique LCLids
lclid_lt6 = test_data6['LCLid'].unique()
sel_lclids6 = lclid_lt6[:1]
# Filter data for the selected LCLids
ft_data6 = test_data6[test_data6['LCLid'].isin(sel_lclids6)]

# Select unique LCLids
lclid_lt7 = test_data7['LCLid'].unique()
sel_lclids7 = lclid_lt7[:1]
# Filter data for the selected LCLids
ft_data7 = test_data7[test_data7['LCLid'].isin(sel_lclids7)]

# # Select unique LCLids
# lclid_lt8 = test_data8['LCLid'].unique()
# sel_lclids8 = lclid_lt8[:1]
# # Filter data for the selected LCLids
# ft_data8 = test_data8[test_data8['LCLid'].isin(sel_lclids8)]

# # Select unique LCLids
# lclid_lt9 = test_data9['LCLid'].unique()
# sel_lclids9 = lclid_lt9[:1]
# # Filter data for the selected LCLids
# ft_data9 = test_data9[test_data9['LCLid'].isin(sel_lclids9)]

# # Select unique LCLids
# lclid_lt10 = test_data10['LCLid'].unique()
# sel_lclids10 = lclid_lt10[:1]
# # Filter data for the selected LCLids
# ft_data10 = test_data10[test_data10['LCLid'].isin(sel_lclids10)]

# # Select unique LCLids
# lclid_lt11 = test_data11['LCLid'].unique()
# sel_lclids11 = lclid_lt11[:1]
# # Filter data for the selected LCLids
# ft_data11 = test_data11[test_data11['LCLid'].isin(sel_lclids11)]

# # Select unique LCLids
# lclid_lt12 = test_data12['LCLid'].unique()
# sel_lclids12 = lclid_lt12[:1]
# # Filter data for the selected LCLids
# ft_data12 = test_data12[test_data12['LCLid'].isin(sel_lclids12)]

ft_data1.reset_index(drop=True, inplace=True)
ft_data2.reset_index(drop=True, inplace=True)
ft_data3.reset_index(drop=True, inplace=True)
ft_data4.reset_index(drop=True, inplace=True)
ft_data5.reset_index(drop=True, inplace=True)
ft_data6.reset_index(drop=True, inplace=True)
ft_data7.reset_index(drop=True, inplace=True)
# ft_data8.reset_index(drop=True, inplace=True)
# ft_data9.reset_index(drop=True, inplace=True)
# ft_data10.reset_index(drop=True, inplace=True)
# ft_data11.reset_index(drop=True, inplace=True)
# ft_data12.reset_index(drop=True, inplace=True)

ft_data1['KWH/hh'] = ft_data1['KWH/hh'].astype(np.float32)
ft_data2['KWH/hh'] = ft_data2['KWH/hh'].astype(np.float32)
ft_data3['KWH/hh'] = ft_data3['KWH/hh'].astype(np.float32)
ft_data4['KWH/hh'] = ft_data4['KWH/hh'].astype(np.float32)
ft_data5['KWH/hh'] = ft_data5['KWH/hh'].astype(np.float32)
ft_data6['KWH/hh'] = ft_data6['KWH/hh'].astype(np.float32)
ft_data7['KWH/hh'] = ft_data7['KWH/hh'].astype(np.float32)
# ft_data8['KWH/hh'] = ft_data8['KWH/hh'].astype(np.float32)
# ft_data9['KWH/hh'] = ft_data9['KWH/hh'].astype(np.float32)
# ft_data10['KWH/hh'] = ft_data10['KWH/hh'].astype(np.float32)
# ft_data11['KWH/hh'] = ft_data11['KWH/hh'].astype(np.float32)
# ft_data12['KWH/hh'] = ft_data12['KWH/hh'].astype(np.float32)

ft_data1 = ft_data1.drop('cluster', axis=1)
ft_data1 = ft_data1.drop('stdorToU', axis=1)

ft_data2 = ft_data2.drop('cluster', axis=1)
ft_data2 = ft_data2.drop('stdorToU', axis=1)

ft_data3 = ft_data3.drop('cluster', axis=1)
ft_data3 = ft_data3.drop('stdorToU', axis=1)

ft_data4 = ft_data4.drop('cluster', axis=1)
ft_data4 = ft_data4.drop('stdorToU', axis=1)

ft_data5 = ft_data5.drop('cluster', axis=1)
ft_data5 = ft_data5.drop('stdorToU', axis=1)

ft_data6 = ft_data6.drop('cluster', axis=1)
ft_data6 = ft_data6.drop('stdorToU', axis=1)

ft_data7 = ft_data7.drop('cluster', axis=1)
ft_data7 = ft_data7.drop('stdorToU', axis=1)

# ft_data8 = ft_data8.drop('cluster', axis=1)
# ft_data8 = ft_data8.drop('stdorToU', axis=1)

# ft_data9 = ft_data9.drop('cluster', axis=1)
# ft_data9 = ft_data9.drop('stdorToU', axis=1)

# ft_data10 = ft_data10.drop('cluster', axis=1)
# ft_data10 = ft_data10.drop('stdorToU', axis=1)

# ft_data11 = ft_data11.drop('cluster', axis=1)
# ft_data11 = ft_data11.drop('stdorToU', axis=1)

# ft_data12 = ft_data12.drop('cluster', axis=1)
# ft_data12 = ft_data12.drop('stdorToU', axis=1)

ft_data1['DateTime'] = pd.to_datetime(ft_data1.DateTime).dt.tz_localize(None)
for i in range(len(ft_data1)):
  ft_data1['DateTime'][i]=ft_data1['DateTime'][i].timestamp()

ft_data2['DateTime'] = pd.to_datetime(ft_data2.DateTime).dt.tz_localize(None)
for i in range(len(ft_data2)):
  ft_data2['DateTime'][i]=ft_data2['DateTime'][i].timestamp()

ft_data3['DateTime'] = pd.to_datetime(ft_data3.DateTime).dt.tz_localize(None)
for i in range(len(ft_data3)):
  ft_data3['DateTime'][i]=ft_data3['DateTime'][i].timestamp()

ft_data4['DateTime'] = pd.to_datetime(ft_data4.DateTime).dt.tz_localize(None)
for i in range(len(ft_data4)):
  ft_data4['DateTime'][i]=ft_data4['DateTime'][i].timestamp()

ft_data5['DateTime'] = pd.to_datetime(ft_data5.DateTime).dt.tz_localize(None)
for i in range(len(ft_data5)):
  ft_data5['DateTime'][i]=ft_data5['DateTime'][i].timestamp()

ft_data6['DateTime'] = pd.to_datetime(ft_data6.DateTime).dt.tz_localize(None)
for i in range(len(ft_data6)):
  ft_data6['DateTime'][i]=ft_data6['DateTime'][i].timestamp()

ft_data7['DateTime'] = pd.to_datetime(ft_data7.DateTime).dt.tz_localize(None)
for i in range(len(ft_data7)):
  ft_data7['DateTime'][i]=ft_data7['DateTime'][i].timestamp()

# ft_data8['DateTime'] = pd.to_datetime(ft_data8.DateTime).dt.tz_localize(None)
# for i in range(len(ft_data8)):
#   ft_data8['DateTime'][i]=ft_data8['DateTime'][i].timestamp()

# ft_data9['DateTime'] = pd.to_datetime(ft_data9.DateTime).dt.tz_localize(None)
# for i in range(len(ft_data9)):
#   ft_data9['DateTime'][i]=ft_data9['DateTime'][i].timestamp()

# ft_data10['DateTime'] = pd.to_datetime(ft_data10.DateTime).dt.tz_localize(None)
# for i in range(len(ft_data10)):
#   ft_data10['DateTime'][i]=ft_data10['DateTime'][i].timestamp()

# ft_data11['DateTime'] = pd.to_datetime(ft_data11.DateTime).dt.tz_localize(None)
# for i in range(len(ft_data11)):
#   ft_data11['DateTime'][i]=ft_data11['DateTime'][i].timestamp()

# ft_data12['DateTime'] = pd.to_datetime(ft_data12.DateTime).dt.tz_localize(None)
# for i in range(len(ft_data12)):
#   ft_data12['DateTime'][i]=ft_data12['DateTime'][i].timestamp()

ft_data1.dtypes

ft_data1['DateTime'] = ft_data1['DateTime'].astype(np.float32)
ft_data2['DateTime'] = ft_data2['DateTime'].astype(np.float32)
ft_data3['DateTime'] = ft_data3['DateTime'].astype(np.float32)
ft_data4['DateTime'] = ft_data4['DateTime'].astype(np.float32)
ft_data5['DateTime'] = ft_data5['DateTime'].astype(np.float32)
ft_data6['DateTime'] = ft_data6['DateTime'].astype(np.float32)
ft_data7['DateTime'] = ft_data7['DateTime'].astype(np.float32)
# ft_data8['DateTime'] = ft_data8['DateTime'].astype(np.float32)
# ft_data9['DateTime'] = ft_data9['DateTime'].astype(np.float32)
# ft_data10['DateTime'] = ft_data10['DateTime'].astype(np.float32)
# ft_data11['DateTime'] = ft_data11['DateTime'].astype(np.float32)
# ft_data12['DateTime'] = ft_data12['DateTime'].astype(np.float32)

# Sort the data by 'LCLid' and 'DateTime'
ft_data1.sort_values(['LCLid', 'DateTime'], inplace=True)
ft_data2.sort_values(['LCLid', 'DateTime'], inplace=True)
ft_data3.sort_values(['LCLid', 'DateTime'], inplace=True)
ft_data4.sort_values(['LCLid', 'DateTime'], inplace=True)
ft_data5.sort_values(['LCLid', 'DateTime'], inplace=True)
ft_data6.sort_values(['LCLid', 'DateTime'], inplace=True)
ft_data7.sort_values(['LCLid', 'DateTime'], inplace=True)
# ft_data8.sort_values(['LCLid', 'DateTime'], inplace=True)
# ft_data9.sort_values(['LCLid', 'DateTime'], inplace=True)
# ft_data10.sort_values(['LCLid', 'DateTime'], inplace=True)
# ft_data11.sort_values(['LCLid', 'DateTime'], inplace=True)
# ft_data12.sort_values(['LCLid', 'DateTime'], inplace=True)

test_client = ft_data1['LCLid'].unique()

test_client

test_dataset1 = {}

for LCLid in test_client:
    # Filter the dataframe for the current LCLid
    client_data = ft_data1[ft_data1['LCLid'] == LCLid]

    clientxx_dataset = create_client_dataset_for_LCLid(client_data, window_size, step_size)

    # Create the client dataset for the current LCLid
    preprocessed_client_dataset = preprocess_client_dataset(tf.data.Dataset.from_tensor_slices(clientxx_dataset))

    # Extract a sample batch from the preprocessed dataset
    sam_batch = tf.nest.map_structure(lambda x: x.numpy(), next(iter(preprocessed_client_dataset)))

    # Store the preprocessed dataset in the dictionary with LCLid as the key
    test_dataset1[LCLid] = preprocessed_client_dataset

    print("Client dataset for LCLid", LCLid)
    print(sam_batch)

test_dataset2 = {}

for LCLid in test_client:
    # Filter the dataframe for the current LCLid
    client_data = ft_data2[ft_data2['LCLid'] == LCLid]

    clientxx_dataset = create_client_dataset_for_LCLid(client_data, window_size, step_size)

    # Create the client dataset for the current LCLid
    preprocessed_client_dataset = preprocess_client_dataset(tf.data.Dataset.from_tensor_slices(clientxx_dataset))

    # Extract a sample batch from the preprocessed dataset
    sam_batch = tf.nest.map_structure(lambda x: x.numpy(), next(iter(preprocessed_client_dataset)))

    # Store the preprocessed dataset in the dictionary with LCLid as the key
    test_dataset2[LCLid] = preprocessed_client_dataset

test_dataset3 = {}

for LCLid in test_client:
    # Filter the dataframe for the current LCLid
    client_data = ft_data3[ft_data3['LCLid'] == LCLid]

    clientxx_dataset = create_client_dataset_for_LCLid(client_data, window_size, step_size)

    # Create the client dataset for the current LCLid
    preprocessed_client_dataset = preprocess_client_dataset(tf.data.Dataset.from_tensor_slices(clientxx_dataset))

    # Extract a sample batch from the preprocessed dataset
    sam_batch = tf.nest.map_structure(lambda x: x.numpy(), next(iter(preprocessed_client_dataset)))

    # Store the preprocessed dataset in the dictionary with LCLid as the key
    test_dataset3[LCLid] = preprocessed_client_dataset

test_dataset4 = {}

for LCLid in test_client:
    # Filter the dataframe for the current LCLid
    client_data = ft_data4[ft_data4['LCLid'] == LCLid]

    clientxx_dataset = create_client_dataset_for_LCLid(client_data, window_size, step_size)

    # Create the client dataset for the current LCLid
    preprocessed_client_dataset = preprocess_client_dataset(tf.data.Dataset.from_tensor_slices(clientxx_dataset))

    # Extract a sample batch from the preprocessed dataset
    sam_batch = tf.nest.map_structure(lambda x: x.numpy(), next(iter(preprocessed_client_dataset)))

    # Store the preprocessed dataset in the dictionary with LCLid as the key
    test_dataset4[LCLid] = preprocessed_client_dataset

test_dataset5 = {}

for LCLid in test_client:
    # Filter the dataframe for the current LCLid
    client_data = ft_data5[ft_data5['LCLid'] == LCLid]

    clientxx_dataset = create_client_dataset_for_LCLid(client_data, window_size, step_size)

    # Create the client dataset for the current LCLid
    preprocessed_client_dataset = preprocess_client_dataset(tf.data.Dataset.from_tensor_slices(clientxx_dataset))

    # Extract a sample batch from the preprocessed dataset
    sam_batch = tf.nest.map_structure(lambda x: x.numpy(), next(iter(preprocessed_client_dataset)))

    # Store the preprocessed dataset in the dictionary with LCLid as the key
    test_dataset5[LCLid] = preprocessed_client_dataset

test_dataset6 = {}

for LCLid in test_client:
    # Filter the dataframe for the current LCLid
    client_data = ft_data6[ft_data6['LCLid'] == LCLid]

    clientxx_dataset = create_client_dataset_for_LCLid(client_data, window_size, step_size)

    # Create the client dataset for the current LCLid
    preprocessed_client_dataset = preprocess_client_dataset(tf.data.Dataset.from_tensor_slices(clientxx_dataset))

    # Extract a sample batch from the preprocessed dataset
    sam_batch = tf.nest.map_structure(lambda x: x.numpy(), next(iter(preprocessed_client_dataset)))

    # Store the preprocessed dataset in the dictionary with LCLid as the key
    test_dataset6[LCLid] = preprocessed_client_dataset

test_dataset7 = {}

for LCLid in test_client:
    # Filter the dataframe for the current LCLid
    client_data = ft_data7[ft_data7['LCLid'] == LCLid]

    clientxx_dataset = create_client_dataset_for_LCLid(client_data, window_size, step_size)

    # Create the client dataset for the current LCLid
    preprocessed_client_dataset = preprocess_client_dataset(tf.data.Dataset.from_tensor_slices(clientxx_dataset))

    # Extract a sample batch from the preprocessed dataset
    sam_batch = tf.nest.map_structure(lambda x: x.numpy(), next(iter(preprocessed_client_dataset)))

    # Store the preprocessed dataset in the dictionary with LCLid as the key
    test_dataset7[LCLid] = preprocessed_client_dataset

# test_dataset8 = {}

# for LCLid in test_client:
#     # Filter the dataframe for the current LCLid
#     client_data = ft_data8[ft_data8['LCLid'] == LCLid]

#     clientxx_dataset = create_client_dataset_for_LCLid(client_data, window_size, step_size)

#     # Create the client dataset for the current LCLid
#     preprocessed_client_dataset = preprocess_client_dataset(tf.data.Dataset.from_tensor_slices(clientxx_dataset))

#     # Extract a sample batch from the preprocessed dataset
#     sam_batch = tf.nest.map_structure(lambda x: x.numpy(), next(iter(preprocessed_client_dataset)))

#     # Store the preprocessed dataset in the dictionary with LCLid as the key
#     test_dataset8[LCLid] = preprocessed_client_dataset

# test_dataset9 = {}

# for LCLid in test_client:
#     # Filter the dataframe for the current LCLid
#     client_data = ft_data9[ft_data9['LCLid'] == LCLid]

#     clientxx_dataset = create_client_dataset_for_LCLid(client_data, window_size, step_size)

#     # Create the client dataset for the current LCLid
#     preprocessed_client_dataset = preprocess_client_dataset(tf.data.Dataset.from_tensor_slices(clientxx_dataset))

#     # Extract a sample batch from the preprocessed dataset
#     sam_batch = tf.nest.map_structure(lambda x: x.numpy(), next(iter(preprocessed_client_dataset)))

#     # Store the preprocessed dataset in the dictionary with LCLid as the key
#     test_dataset9[LCLid] = preprocessed_client_dataset

# test_dataset10 = {}

# for LCLid in test_client:
#     # Filter the dataframe for the current LCLid
#     client_data = ft_data10[ft_data10['LCLid'] == LCLid]

#     clientxx_dataset = create_client_dataset_for_LCLid(client_data, window_size, step_size)

#     # Create the client dataset for the current LCLid
#     preprocessed_client_dataset = preprocess_client_dataset(tf.data.Dataset.from_tensor_slices(clientxx_dataset))

#     # Extract a sample batch from the preprocessed dataset
#     sam_batch = tf.nest.map_structure(lambda x: x.numpy(), next(iter(preprocessed_client_dataset)))

#     # Store the preprocessed dataset in the dictionary with LCLid as the key
#     test_dataset10[LCLid] = preprocessed_client_dataset

# test_dataset11 = {}

# for LCLid in test_client:
#     # Filter the dataframe for the current LCLid
#     client_data = ft_data11[ft_data11['LCLid'] == LCLid]

#     clientxx_dataset = create_client_dataset_for_LCLid(client_data, window_size, step_size)

#     # Create the client dataset for the current LCLid
#     preprocessed_client_dataset = preprocess_client_dataset(tf.data.Dataset.from_tensor_slices(clientxx_dataset))

#     # Extract a sample batch from the preprocessed dataset
#     sam_batch = tf.nest.map_structure(lambda x: x.numpy(), next(iter(preprocessed_client_dataset)))

#     # Store the preprocessed dataset in the dictionary with LCLid as the key
#     test_dataset11[LCLid] = preprocessed_client_dataset

# test_dataset12 = {}

# for LCLid in test_client:
#     # Filter the dataframe for the current LCLid
#     client_data = ft_data12[ft_data12['LCLid'] == LCLid]

#     clientxx_dataset = create_client_dataset_for_LCLid(client_data, window_size, step_size)

#     # Create the client dataset for the current LCLid
#     preprocessed_client_dataset = preprocess_client_dataset(tf.data.Dataset.from_tensor_slices(clientxx_dataset))

#     # Extract a sample batch from the preprocessed dataset
#     sam_batch = tf.nest.map_structure(lambda x: x.numpy(), next(iter(preprocessed_client_dataset)))

#     # Store the preprocessed dataset in the dictionary with LCLid as the key
#     test_dataset12[LCLid] = preprocessed_client_dataset

# Make federated test data
federated_test_data1 = make_federated_data(test_dataset1, test_client)
federated_test_data2 = make_federated_data(test_dataset2, test_client)
federated_test_data3 = make_federated_data(test_dataset3, test_client)
federated_test_data4 = make_federated_data(test_dataset4, test_client)
federated_test_data5 = make_federated_data(test_dataset5, test_client)
federated_test_data6 = make_federated_data(test_dataset6, test_client)
federated_test_data7 = make_federated_data(test_dataset7, test_client)
# federated_test_data8 = make_federated_data(test_dataset8, test_client)
# federated_test_data9 = make_federated_data(test_dataset9, test_client)
# federated_test_data10 = make_federated_data(test_dataset10, test_client)
# federated_test_data11 = make_federated_data(test_dataset11, test_client)
# federated_test_data12 = make_federated_data(test_dataset12, test_client)

evaluation_process = tff.learning.algorithms.build_fed_eval(model_fn)

print(evaluation_process.next.type_signature.formatted_representation())

evaluation_state = evaluation_process.initialize()
model_weights = training_process.get_model_weights(train_state)
evaluation_state = evaluation_process.set_model_weights(evaluation_state, model_weights)

evaluation_output1 = evaluation_process.next(evaluation_state, federated_test_data1)
evaluation_output2 = evaluation_process.next(evaluation_state, federated_test_data2)
evaluation_output3 = evaluation_process.next(evaluation_state, federated_test_data3)
evaluation_output4 = evaluation_process.next(evaluation_state, federated_test_data4)
evaluation_output5 = evaluation_process.next(evaluation_state, federated_test_data5)
evaluation_output6 = evaluation_process.next(evaluation_state, federated_test_data6)
evaluation_output7 = evaluation_process.next(evaluation_state, federated_test_data7)
# evaluation_output8 = evaluation_process.next(evaluation_state, federated_test_data8)
# evaluation_output9 = evaluation_process.next(evaluation_state, federated_test_data9)
# evaluation_output10 = evaluation_process.next(evaluation_state, federated_test_data10)
# evaluation_output11 = evaluation_process.next(evaluation_state, federated_test_data11)
# evaluation_output12 = evaluation_process.next(evaluation_state, federated_test_data12)

loss_values_1 = []

# Append MAE values to the list
loss_values_1.append(evaluation_output1.metrics['client_work']['eval']['current_round_metrics']['loss'])
loss_values_1.append(evaluation_output2.metrics['client_work']['eval']['current_round_metrics']['loss'])
loss_values_1.append(evaluation_output3.metrics['client_work']['eval']['current_round_metrics']['loss'])
loss_values_1.append(evaluation_output4.metrics['client_work']['eval']['current_round_metrics']['loss'])
loss_values_1.append(evaluation_output5.metrics['client_work']['eval']['current_round_metrics']['loss'])
loss_values_1.append(evaluation_output6.metrics['client_work']['eval']['current_round_metrics']['loss'])
loss_values_1.append(evaluation_output7.metrics['client_work']['eval']['current_round_metrics']['loss'])
# loss_values_1.append(evaluation_output8.metrics['client_work']['eval']['current_round_metrics']['loss'])
# loss_values_1.append(evaluation_output9.metrics['client_work']['eval']['current_round_metrics']['loss'])
# loss_values_1.append(evaluation_output10.metrics['client_work']['eval']['current_round_metrics']['loss'])
# loss_values_1.append(evaluation_output11.metrics['client_work']['eval']['current_round_metrics']['loss'])
# loss_values_1.append(evaluation_output12.metrics['client_work']['eval']['current_round_metrics']['loss'])


# Print the MAE values
print(loss_values_1)

loss_values_2 = []

# Append MAE values to the list
loss_values_2.append(evaluation_output1.metrics['client_work']['eval']['current_round_metrics']['root_mean_squared_error'])
loss_values_2.append(evaluation_output2.metrics['client_work']['eval']['current_round_metrics']['root_mean_squared_error'])
loss_values_2.append(evaluation_output3.metrics['client_work']['eval']['current_round_metrics']['root_mean_squared_error'])
loss_values_2.append(evaluation_output4.metrics['client_work']['eval']['current_round_metrics']['root_mean_squared_error'])
loss_values_2.append(evaluation_output5.metrics['client_work']['eval']['current_round_metrics']['root_mean_squared_error'])
loss_values_2.append(evaluation_output6.metrics['client_work']['eval']['current_round_metrics']['root_mean_squared_error'])
loss_values_2.append(evaluation_output7.metrics['client_work']['eval']['current_round_metrics']['root_mean_squared_error'])
# loss_values_2.append(evaluation_output8.metrics['client_work']['eval']['current_round_metrics']['root_mean_squared_error'])
# loss_values_2.append(evaluation_output9.metrics['client_work']['eval']['current_round_metrics']['root_mean_squared_error'])
# loss_values_2.append(evaluation_output10.metrics['client_work']['eval']['current_round_metrics']['root_mean_squared_error'])
# loss_values_2.append(evaluation_output11.metrics['client_work']['eval']['current_round_metrics']['root_mean_squared_error'])
# loss_values_2.append(evaluation_output12.metrics['client_work']['eval']['current_round_metrics']['root_mean_squared_error'])


# Print the MAE values
print(loss_values_2)

import plotly.graph_objects as go
import numpy as np

# Data
dates = ['Feb 22', 'Feb 23', 'Feb 24', 'Feb 25', 'Feb 26', 'Feb 27', 'Feb 28']
rmse_values = [0.07503, 0.11120, 0.12959, 0.03374, 0.11431, 0.08411 ,0.00963]

# Calculate mean
mean_rmse = np.mean(rmse_values)

# Creating the plot
fig = go.Figure()

# Adding the trace for RMSE values
fig.add_trace(go.Scatter(x=dates, y=rmse_values, mode='lines+markers', name='RMSE'))

# Adding a line for mean RMSE
fig.add_shape(
    type="line",
    x0=dates[0],
    y0=mean_rmse,
    x1=dates[-1],
    y1=mean_rmse,
    line=dict(color='red', dash='dash'),
    name='Mean RMSE'
)

# Updating the axis labels and title
fig.update_layout(
    xaxis_title='Date',
    yaxis_title='RMSE',
    title='Weekly Forecast (Feb 22 - Feb 28) of a Moderate Consumer - Cluster 18'
)

# Display the plot
fig.show()